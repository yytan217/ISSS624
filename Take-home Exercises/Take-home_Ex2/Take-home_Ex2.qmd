---
title: "Take-home Exercise 2: Regionalisation of Multivariate Water Point Attributes with Non-spatially Constrained and Spatially Constrained Clustering Methods"
author: Yi Ying TAN
execute: 
  warning: false
  message: false
editor: visual
---

## Overview

## Getting Started

### The Data

We will continue our work from Take-home Exercise 1 and will be using the same data sources :

1.  Water point geospatial data - obtained from [WPDx Global Data Repositories](https://www.waterpointdata.org/access-data/) - The repository is a cloud based data library that stores the water point related data from rural areas at the water point or small water scheme level collected by the WPDx project.
2.  Nigeria LGA boundary data - Nigeria Level-2 Administrative Boundary (also known as Local Government Area) polygon features GIS data downloaded from [geoBoundaries](https://www.geoboundaries.org/) website.

### Setting the tools

We start by ensuring we have all the required R packages installed and loaded. The few key packages used and their purposes as follows:

-   **sf, rgdal** and **spdep** - spatial data handling

-   **tidyverse**, especially **readr**, **ggplot2** and **dplyr** - attribute data handling

-   **tmap** - choropleth mapping

-   **coorplot**, **ggpubr**, and **heatmaply** - multivariate data visualization and analysis

-   **cluster** and **ClustGeo** - cluster analysis

-   **funModeling** - for quick Exploratory Data Analysis.

The code chunk below installs and loads these R packages.

```{r}
pacman::p_load(rgdal, spdep, tmap, sf, funModeling,
               ggpubr, cluster, factoextra, NbClust,
               heatmaply, corrplot, psych, tidyverse, ClustGeo)
```

## Data Preparation

### Importing water point geospatial data

```{r}
#| eval: false
wp <- st_read(dsn = "geodata", 
              layer = "geo_export",
              crs = 4326) %>%
  filter(clean_coun == "Nigeria") 
```

Next, we save the extracted sf data table into an output file in rds data format and save in the Geodata file.

```{r}
#| eval: false
write_rds(wp,"geodata/wp_nga.rds")
```

### Importing Nigeria LGA boundary data

```{r}
#| eval: false
nga <- st_read(dsn = "geodata",
               layer = "geoBoundaries-NGA-ADM2",
               crs = 4326)
```

### Data Wrangling

We see many NA in the *status_cle* field, below code chunk is used to recode all *NA* status into *Unknown*.

```{r}
#| eval: false
wp_nga <- read_rds("geodata/wp_nga.rds") %>%
  rename ("Country" = "clean_coun",
          "clean_adm2" = "clean_adm2",
          "lat" = "lat_deg",
          "long" = "lon_deg") %>%
  mutate(status_cle = replace_na(status_cle, "Unknown"))

wp_nga_sf <- st_as_sf(wp_nga, coords = c("long", "lat"),  crs = 4326)
```

### Projection Transformation

Next, we transform the coordinates from 4326 to 26391 projection using *st_transform()* function.

```{r}
#| eval: false
wp_nga_sf <- st_transform(wp_nga_sf, crs = 26391)
nga <- nga %>%
  st_transform(crs = 26391)

st_crs (nga)
st_crs (wp_nga_sf)
```

### Visualizing water point distribution

We can use the simple function *freq()* of funModeling package to display the distribution of *status_cle* field.

```{r}
#| eval: false
freq(data=wp_nga_sf,
     input = 'status_cle')
```

### Extracting Water Point Data

#### Extracting functional water point

If we look at the first EDA results, we know that there are a few that falls under the 'functional' category.

```{r}
#| eval: false
wpt_functional <- wp_nga_sf %>%
  filter(status_cle %in%
           c("Functional", 
             "Functional but not in use",
             "Functional but needs repair"))
```

```{r}
#| eval: false
freq(data=wpt_functional,
     input = 'status_cle')
```

#### Extracting non-functional water point

```{r}
#| eval: false
wpt_nonfunctional <- wp_nga_sf %>%
  filter(status_cle %in%
           c("Non-Functional", 
             "Non-Functional due to dry season",
             "Abandoned/Decommissioned",
             "Abandoned",
             "Non functional due to dry season"))
```

```{r}
#| eval: false
freq(data=wpt_nonfunctional,
     input = 'status_cle')
```

#### Extracting water point with unknown status

```{r}
#| eval: false
wpt_unknown <- wp_nga_sf %>%
  filter(status_cle %in%
           c("Unknown"))
```

Looking at the data, the results is the same as the first EDA, there are 10656 water point with unknown status.

#### Extracting water point using the main technology

```{r}
#| eval: false
wpt_handpump <- wp_nga_sf %>%
  filter(X_water_tec %in%
           c("Hand Pump"))
```

#### Extracting the usage capacity of water points

```{r}
#| eval: false
wpt_lowuse <- wp_nga_sf %>%
  filter(usage_cap < 1000)
```

```{r}
#| eval: false
wpt_highuse <- wp_nga_sf %>%
  filter(usage_cap >= 1000)
```

#### Extracting rural water points

```{r}
#| eval: false
wpt_rural <- wp_nga_sf %>%
  filter(is_urban %in%
           c("False"))
```

### Performing Point-in-Polygon Count

```{r}
#| eval: false
nga_wp <- nga %>% 
  mutate(`total wpt` = lengths(
    st_intersects(nga, wp_nga_sf))) %>%
  mutate(`wpt functional` = lengths(
    st_intersects(nga, wpt_functional))) %>%
  mutate(`wpt non-functional` = lengths(
    st_intersects(nga, wpt_nonfunctional))) %>%
  mutate(`wpt unknown` = lengths(
    st_intersects(nga, wpt_unknown))) %>%
  mutate(`wpt hand pump` = lengths(
    st_intersects(nga, wpt_handpump))) %>%
  mutate(`wpt high usage` = lengths(
    st_intersects(nga, wpt_highuse))) %>%
  mutate(`wpt low usage` = lengths(
    st_intersects(nga, wpt_lowuse))) %>% 
  mutate(`wpt rural` = lengths(
    st_intersects(nga, wpt_rural)))  
```

### Saving the Analytical Data Table

```{r}
#| eval: false
nga_wp <- nga_wp %>%
  mutate(`pct_functional` = `wpt functional`/`total wpt`) %>%
  mutate(`pct_non_functional` = `wpt non-functional`/`total wpt`) %>%
  mutate(`pct_handpump` = `wpt hand pump`/`total wpt`) %>%
  mutate(`pct_highusage` = `wpt high usage`/`total wpt`) %>%
  mutate(`pct_lowusage` = `wpt low usage`/`total wpt`) %>%
  mutate(`pct_rural` = `wpt rural`/`total wpt`)
```

Saving the sf data table with analysis into rds format.

```{r}
#| eval: false
write_rds(nga_wp, "geodata/nga_wp.rds")
```

```{r}
#| eval: false
nga_wp <- read_rds("geodata/nga_wp.rds") %>%
  mutate(pct_functional = replace_na(pct_functional, 0)) %>%
  mutate(pct_non_functional = replace_na(pct_non_functional, 0)) %>%
  mutate(pct_handpump = replace_na(pct_handpump, 0)) %>%
  mutate(pct_highusage = replace_na(pct_highusage, 0)) %>%
  mutate(pct_lowusage = replace_na(pct_lowusage, 0)) %>% 
  mutate(pct_rural = replace_na(pct_rural, 0)) 
```

## Exploratory Data Analysis

### EDA using statistical graphs

We will use histogram to look at the overall distribution of the data values.

```{r}
#| eval: false
functional <- ggplot(data=nga_wp, 
             aes(x= `wpt functional`)) +
  geom_histogram(bins=20, 
                 color="black", 
                 fill="green")

nonfunctional <- ggplot(data=nga_wp, 
             aes(x= `wpt non-functional`)) +
  geom_histogram(bins=20, 
                 color="black", 
                 fill="green")

handpump <- ggplot(data=nga_wp, 
             aes(x= `wpt hand pump`)) +
  geom_histogram(bins=20, 
                 color="black", 
                 fill="green")

highuse <- ggplot(data=nga_wp, 
             aes(x= `wpt high usage`)) +
  geom_histogram(bins=20, 
                 color="black", 
                 fill="green")

lowuse <- ggplot(data=nga_wp, 
             aes(x= `wpt low usage`)) +
  geom_histogram(bins=20, 
                 color="black", 
                 fill="green")

rural <- ggplot(data=nga_wp, 
             aes(x= `wpt rural`)) +
  geom_histogram(bins=20, 
                 color="black", 
                 fill="green")
```

```{r}
#| eval: false
ggarrange(functional, nonfunctional, handpump, highuse, lowuse, rural, 
          ncol = 3, 
          nrow = 2)
```

We see that the value range for the variables are quite similar, as such we do not need to standardize the input variables.

## Correlation Analysis

```{r}
#| eval: false
cluster_vars <- nga_wp %>%
  st_set_geometry(NULL)
cluster_vars.cor = cor(cluster_vars[,14:19])
corrplot.mixed(cluster_vars.cor,
         lower = "ellipse", 
               upper = "number",
               tl.pos = "lt",
               diag = "l",
               tl.col = "black")
```

The correlation plot shows that pct_highusage and pct_lowusage are highly correlated (understandably given these 2 variables should add up to be 1), we will remove one of them and only use pct_highusage in the cluster analysis.

## Hierarchy Cluster Analysis

### Extracting clustering variables

```{r}
#| eval: false
cluster_vars <- nga_wp %>%
  st_set_geometry(NULL) %>%
  select("shapeName", "pct_functional", "pct_non_functional", "pct_handpump", "pct_highusage", "pct_rural")
head(cluster_vars,10)
```

Notice we have not included pct_lowusage because it is highly correlated with pct_highusage.

Next, we will change the row numbers to township name by using code chunk below.

```{r}
#| eval: false
row.names(cluster_vars) <- make.names(cluster_vars$"shapeName", unique = TRUE)
head(cluster_vars,10)
```

We can see that the row numbers are now replaced by unique township names.

Now, we will delete the shapeName field.

```{r}
#| eval: false
nga_wp_cv <- select(cluster_vars,c(2:6))
head(nga_wp_cv, 10)
```

### Computing proximity matrix

We will compute the proximity matrix by using *dist()* of R. *dist()* supports 6 distance proximity calculations: **euclidean** (default)**, maximum, manhattan, canberra, binary** and **minkowski**.

```{r}
#| eval: false
proxmat <- dist(nga_wp_cv, method = 'euclidean')
```

We will inspect the content of proxmat using this code chunk.

```{r}
#| eval: false
proxmat
```

### Computing hierarchical clustering

The code chunk below performs hierarchical cluster analysis using ward.D method. The hierarchical clustering output is stored in an object of class **hclust** which describes the tree produced by the clustering process.

```{r}
#| eval: false
hclust_ward <- hclust(proxmat, method = 'ward.D')
```

Now, we can plot the tree.

```{r}
#| eval: false
plot(hclust_ward, cex = 0.3)
```

### Selecting the optimal clustering algorithm

The identification of stronger clustering structures is one of the challenges we face when performing hierarchical clustering. We can solve this using *agnes()* function of **cluster** package, which will compute the agglomerative coefficients of all hierarchical clustering algorithms. (Values closer to 1 suggest strong clustering structure)

```{r}
#| eval: false
m <- c( "average", "single", "complete", "ward")
names(m) <- c( "average", "single", "complete", "ward")

ac <- function(x) {
  agnes(nga_wp_cv, method = x)$ac
}

map_dbl(m, ac)
```

We see from the results that Ward's method will provide the strongest clustering structure, hence we will be using Ward for the subsequent analysis in this exercise.

### Determining optimal clusters

There are 3 commonly used methods to determine the optimal clusters:

-   Elbow Method

-   Average Silhouette Method

-   Gap Statistic Method

#### Gap Statistic Method

The gap statistics compares the total within intra-cluster variation for different values of k with their expected values under null reference distribution of the data. The estimate of the optimal clusters will be the value that maximize the gap statistic (i.e., that yields the largest gap statistic). This means that the clustering structure is far away from the random uniform distribution of points.

```{r}
#| eval: false
set.seed(12345)
gap_stat <- clusGap(nga_wp_cv, 
                    FUN = hcut, 
                    nstart = 25, 
                    K.max = 10, 
                    B = 50)
# Print the result
print(gap_stat, method = "firstmax")
```

Next, we can visualize the results.

```{r}
#| eval: false
fviz_gap_stat(gap_stat)
```

If we look at the gap statistic graph, the recommended number of cluster to retain is **9**.

### Interpreting the dendrograms

In the dendrogram above, each leaf corresponds to one observation. As we move up the tree, observations that are similar to each other are combined into branches, which are themselves fused at a higher height.

The height of the fusion, provided on the vertical axis, indicates the similarity (or dissimilarity) between two observations. The higher the height of the fusion, the less similar the observations are.

We can draw the dendrogram with a border around the selected clusters.

```{r}
#| eval: false
plot(hclust_ward, cex = 0.3)
rect.hclust(hclust_ward, 
            k = 9, 
            border = 2:5)
```

### Visually-driven hierarchical clustering analysis

In this section we will build both highly interactive and static cluster heatmaps using **heatmaply**.

#### Transforming the data frame into matrix

```{r}
#| eval: false
nga_wp_cv_mat <- data.matrix(nga_wp_cv)
```

#### Plotting interactive cluster heatmap

```{r}
#| eval: false
heatmaply(normalize(nga_wp_cv_mat),
          Colv=NA,
          dist_method = "euclidean",
          hclust_method = "ward.D",
          seriate = "OLO",
          colors = Greens,
          k_row = 6,
          margins = c(NA,200,60,NA),
          fontsize_row = 4,
          fontsize_col = 5,
          main="Geographic Segmentation of Nigeria by Water Point indicators",
          xlab = "Water Point Indicators",
          ylab = "Townships of Nigeria"
          )
```

### Mapping the clusters found

After examining the dendogram above, we decide to retained 9 clusters.

```{r}
#| eval: false
groups <- as.factor(cutree(hclust_ward, k=9))
```

But to visualize the clusters, the 9 groups need to be appended onto *nga_wp* simple feature object. Below code chunk is used to perform 3 steps:

-   convert the *groups* list object to a matrix

-   Use *cbind()* to append groups matrix onto *nga_wp*

-   rename *as.matrix.groups* field to *CLUSTER*

```{r}
#| eval: false
nga_wp_cluster <- cbind(nga_wp, as.matrix(groups)) %>%
  rename(`CLUSTER`=`as.matrix.groups.`)
```

After, we will plot the choropleth map.

```{r}
#| eval: false
qtm(nga_wp_cluster, "CLUSTER")
```

We see above that the clusters are fragmented. This is one of the common limitations of non-spatial clustering algo such as hierarchical cluster analysis.

## Spatially Constrained Clustering - SKATER

### Converting into spatial polygon data frame

First, we need to convert *nga_wp* into spatial polygon data frame. This is because SKATER function only support **sp** objects. We will use below code chunk for the conversion.

```{r}
#| eval: false
nga_wp_sp <- as_Spatial(nga_wp)
```

### Computing neighbor list

```{r}
#| eval: false
nga_wp.nb <- poly2nb(nga_wp_sp)
summary(nga_wp.nb)
```

Next, we will plot the neighbors list using below code chunk.

```{r}
#| eval: false
plot(nga_wp_sp, 
     border=grey(.5))
plot(nga_wp.nb, 
     coordinates(nga_wp_sp), 
     col="green", 
     add=TRUE)
```

#### Calculating edge costs

The cost of each edge is the distance between its nodes.

```{r}
#| eval: false
lcosts <- nbcosts(nga_wp.nb, nga_wp_cv)
```

For each observation, this gives the pairwise dissimilarity between its values on the five variables and the values for the neighbouring observation (from the neighbour list). Basically, this is the notion of a generalised weight for a spatial weights matrix.

```{r}
#| eval: false
nga_wp.w <- nb2listw(nga_wp.nb, 
                   lcosts, 
                   style="B")
summary(nga_wp.w)
```
